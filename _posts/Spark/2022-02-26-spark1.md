---
title: Spark 개념 및 튜닝
excerpt: Context, Session

categories:
  - Spark

tags:
  - Spark
  - pyspark
  - pysparkSQL
  - Python

toc: true
toc_sticky: true
toc_label: Index
---

지나가다가 spark를 사용하는 중 python for문을 만나는 순간! 바로 스레드 하나에서 위이이이잉 돌아서 성능저하가 일어난다는 글을 보았다 -\_-... 😰

그러다 보니 몇가지 궁금한 점이 생겼는데, 차차 풀어나가고자한다.

for문 내 spark를 입히면 말짱 도루묵인건 알겠다. 그런데 for문으로 만든 **함수**를 map과 같은 spark 메서드를 사용하여 새로운 RDD를 생성하여도 성능저하가 일어날까? 지금 포스트에선 다루지 않을꺼지만, 너무나 궁금하다 ㅋㅋ 스파크에 대한 개념을 조금 이해하고, 테스트 해봐야겠다.

### Spark Context & SparkSession

애플리케이션을 동작하기 위한 서버프로세스의 역할

1.  SparkContext : 클러스터를 구성하는 개별 서버 간 각종 메시지와 필요한 데이터를 주고 받는 백엔드 서버 프로세스, RDD 모델 생성 (Context)
2.  SparkSession : 개별 작업 수행에 필요한 메타 정보 저장하고 관리 및 데이터프레임 생성(SQLContext => SparkSession(spark2.0.0이상))

### 스파크의 작업 수행 방법

-   Driver 란 ? 최초 메인 함수 실행해 RDD 생성하고 각종 연산을 호출하는 프로그램
-   Driver의 main 함수에서는 `SparkContext`와 `SparkSession`이라는 객체를 만드는데, 이들은 스파크 애플리케이션과 클러스터의 연동을 담당하고 잡을 실행하고 종료한다.

1.  전체 작업을 stage 라는 단위로 나누고, 각 stage를 다시 여러개의 task로 나눈다.
2.  Driver가 SparkContext를 통해 RDD의 연산 정보를 DAG 스케줄러(oozie)에 전달하면 스케줄러는 이 정보를 통해 실행 계획을 수립하여 클러스터 매니저에게 전달한다. 이때 스케줄러는 데이터에 대해 지역성(local)을 높이려하는데, 전체 데이터 처리 흐름을 분석해서 네트워크를 통해 데이터 이동이 최소화되도록 stage를 구성한다.

### Dependencies

1.  Narrow Dependencies :